import os
import time
import tempfile
from dotenv import load_dotenv
import nltk
from pymongo import MongoClient
from datetime import datetime
from typing import List, Dict, Tuple, Optional
import hashlib

# ======================
# 1. Chu·∫©n b·ªã NLTK
# ======================
nltk.download("punkt", quiet=True)
try:
    nltk.download("punkt_tab", quiet=True)
except:
    pass

from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import NLTKTextSplitter, RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.schema import Document

# ======================
# Danh s√°ch model h·ª£p l·ªá v·ªõi m√¥ t·∫£
# ======================
VALID_MODELS = {
    "intfloat/multilingual-e5-large-instruct": {
        "name": "Multilingual E5 Large",
        "languages": ["vi", "en", "multi"],
        "dimension": 1024,
        "description": "Model ƒëa ng√¥n ng·ªØ m·∫°nh m·∫Ω, t·ªët cho ti·∫øng Vi·ªát"
    },
    "hiieu/halong_embedding": {
        "name": "Halong Embedding",
        "languages": ["vi"],
        "dimension": 768,
        "description": "Model t·ªëi ∆∞u cho ti·∫øng Vi·ªát"
    },
    "AITeamVN/Vietnamese_Embedding": {
        "name": "Vietnamese Embedding",
        "languages": ["vi"],
        "dimension": 768,
        "description": "Model chuy√™n bi·ªát cho ti·∫øng Vi·ªát"
    }
}

# ======================
# Configuration
# ======================
# Th∆∞ m·ª•c l∆∞u vectorstores (relative to this file)
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
DEFAULT_VECTORSTORES_DIR = os.path.join(PROJECT_ROOT, "vectorstores")


# ======================
# Text Splitter Strategy
# ======================
class TextSplitterStrategy:
    """Strategy pattern cho text splitting"""

    @staticmethod
    def get_splitter(strategy: str, chunk_size: int, chunk_overlap: int):
        if strategy == "nltk":
            return NLTKTextSplitter(
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap,
                separator="\n\n",
            )
        elif strategy == "recursive":
            return RecursiveCharacterTextSplitter(
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap,
                separators=["\n\n", "\n", ". ", " ", ""],
            )
        else:
            raise ValueError(f"Unknown strategy: {strategy}")


# ======================
# Utility Functions
# ======================
def calculate_file_hash(file_path: str) -> str:
    """T√≠nh hash c·ªßa file ƒë·ªÉ detect duplicate"""
    hash_md5 = hashlib.md5()
    with open(file_path, "rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            hash_md5.update(chunk)
    return hash_md5.hexdigest()


def clean_text(text: str) -> str:
    """L√†m s·∫°ch text tr∆∞·ªõc khi embedding"""
    # Lo·∫°i b·ªè whitespace th·ª´a
    text = " ".join(text.split())
    # Lo·∫°i b·ªè c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát kh√¥ng c·∫ßn thi·∫øt
    text = text.replace("\x00", "")
    return text.strip()


def validate_pdf(pdf_path: str) -> Tuple[bool, Optional[str]]:
    """Validate PDF file"""
    if not os.path.exists(pdf_path):
        return False, "File kh√¥ng t·ªìn t·∫°i"

    if not pdf_path.lower().endswith('.pdf'):
        return False, "File kh√¥ng ph·∫£i ƒë·ªãnh d·∫°ng PDF"

    file_size = os.path.getsize(pdf_path)
    max_size = 50 * 1024 * 1024  # 50MB
    if file_size > max_size:
        return False, f"File qu√° l·ªõn (>{max_size / 1024 / 1024}MB)"

    if file_size == 0:
        return False, "File r·ªóng"

    return True, None


def check_duplicate_vectorstore(
        mongo_uri: str,
        file_hash: str,
        model_name: str,
        chunk_size: int,
        chunk_overlap: int
) -> Optional[Dict]:
    """Ki·ªÉm tra xem vectorstore ƒë√£ t·ªìn t·∫°i ch∆∞a"""
    client = MongoClient(mongo_uri)
    db = client["interviewer_ai"]
    collection = db["vectorstores"]

    existing = collection.find_one({
        "file_hash": file_hash,
        "model_name": model_name,
        "chunk_size": chunk_size,
        "chunk_overlap": chunk_overlap
    })

    return existing


# ======================
# Main Function - Improved
# ======================
def build_vectorstore(
        pdf_path: str,
        chunk_size: int = 1600,
        chunk_overlap: int = 400,
        model_name: str = "intfloat/multilingual-e5-large-instruct",
        mongo_uri: str = "mongodb://localhost:27017/",
        splitter_strategy: str = "nltk",
        skip_duplicate: bool = True,
        custom_metadata: Optional[Dict] = None,
        progress_callback: Optional[callable] = None,
        output_dir: Optional[str] = None  # NEW: Cho ph√©p custom output dir
) -> Tuple[str, Dict]:
    """
    Build vectorstore v·ªõi nhi·ªÅu c·∫£i ti·∫øn

    Args:
        pdf_path: ƒê∆∞·ªùng d·∫´n file PDF
        chunk_size: K√≠ch th∆∞·ªõc m·ªói chunk
        chunk_overlap: Overlap gi·ªØa c√°c chunk
        model_name: T√™n model embedding
        mongo_uri: URI MongoDB
        splitter_strategy: Chi·∫øn l∆∞·ª£c chia vƒÉn b·∫£n ('nltk' ho·∫∑c 'recursive')
        skip_duplicate: B·ªè qua n·∫øu ƒë√£ t·ªìn t·∫°i vectorstore gi·ªëng h·ªát
        custom_metadata: Metadata t√πy ch·ªânh
        progress_callback: Callback function ƒë·ªÉ b√°o c√°o ti·∫øn ƒë·ªô

    Returns:
        Tuple[save_path, metadata]
    """

    def report_progress(stage: str, progress: int):
        if progress_callback:
            progress_callback(stage, progress)

    # Validate model
    if model_name not in VALID_MODELS:
        raise ValueError(f"Model {model_name} kh√¥ng h·ª£p l·ªá. Ch·ªçn t·ª´: {list(VALID_MODELS.keys())}")

    # Validate PDF
    is_valid, error_msg = validate_pdf(pdf_path)
    if not is_valid:
        raise ValueError(f"PDF kh√¥ng h·ª£p l·ªá: {error_msg}")

    report_progress("validation", 10)

    # Calculate file hash
    file_hash = calculate_file_hash(pdf_path)

    # Check duplicate
    if skip_duplicate:
        existing = check_duplicate_vectorstore(
            mongo_uri, file_hash, model_name, chunk_size, chunk_overlap
        )
        if existing:
            print(f"‚ö†Ô∏è Vectorstore ƒë√£ t·ªìn t·∫°i: {existing['vectorstore_name']}")
            return existing['vectorstore_path'], existing

    report_progress("loading", 20)

    # Load PDF
    try:
        loader = PyPDFLoader(pdf_path)
        pages = loader.load()
        print(f"üìÑ ƒê√£ load {len(pages)} trang t·ª´ PDF")
    except Exception as e:
        raise ValueError(f"L·ªói khi load PDF: {str(e)}")

    if not pages:
        raise ValueError("PDF kh√¥ng c√≥ n·ªôi dung")

    # Clean v√† merge text
    full_text = "\n".join([clean_text(p.page_content) for p in pages])
    full_doc = [Document(page_content=full_text, metadata={"source": pdf_path})]

    report_progress("splitting", 40)

    # Split text
    text_splitter = TextSplitterStrategy.get_splitter(
        splitter_strategy, chunk_size, chunk_overlap
    )

    splitted_docs = []
    for doc in full_doc:
        chunks = text_splitter.split_text(doc.page_content)
        for i, chunk in enumerate(chunks):
            splitted_docs.append({
                "page_content": clean_text(chunk),
                "metadata": {
                    **doc.metadata,
                    "chunk_index": i,
                    "chunk_size": len(chunk),
                    "splitter": splitter_strategy
                }
            })

    print(f"‚úÇÔ∏è ƒê√£ chia th√†nh {len(splitted_docs)} chunks")

    # Filter empty chunks
    splitted_docs = [d for d in splitted_docs if len(d["page_content"].strip()) > 50]
    print(f"üîç Sau khi l·ªçc: {len(splitted_docs)} chunks h·ª£p l·ªá")

    report_progress("embedding", 60)

    # Create embeddings
    device = "cpu"
    embeddings = HuggingFaceEmbeddings(
        model_name=model_name,
        model_kwargs={"device": device},
        encode_kwargs={"normalize_embeddings": True},
    )

    # Create vectorstore
    try:
        vectorstore = FAISS.from_texts(
            [d["page_content"] for d in splitted_docs],
            embeddings,
            metadatas=[d["metadata"] for d in splitted_docs],
        )
    except Exception as e:
        raise ValueError(f"L·ªói khi t·∫°o vectorstore: {str(e)}")

    report_progress("saving", 80)

    # Save vectorstore
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # S·ª≠ d·ª•ng output_dir n·∫øu c√≥, n·∫øu kh√¥ng d√πng default
    vectorstores_dir = output_dir or DEFAULT_VECTORSTORES_DIR
    os.makedirs(vectorstores_dir, exist_ok=True)

    base_name = os.path.splitext(os.path.basename(pdf_path))[0]
    vs_name = f"vs_{base_name}_{timestamp}"
    save_path = os.path.join(vectorstores_dir, vs_name)

    vectorstore.save_local(save_path)
    print(f"üíæ Vectorstore ƒë√£ l∆∞u t·∫°i: {save_path}")

    # Save metadata to MongoDB
    client = MongoClient(mongo_uri)
    db = client["interviewer_ai"]
    collection = db["vectorstores"]

    metadata = {
        "pdf_file": os.path.basename(pdf_path),
        "pdf_path": pdf_path,
        "file_hash": file_hash,
        "file_size_mb": os.path.getsize(pdf_path) / (1024 * 1024),
        "model_name": model_name,
        "model_info": VALID_MODELS[model_name],
        "chunk_size": chunk_size,
        "chunk_overlap": chunk_overlap,
        "splitter_strategy": splitter_strategy,
        "num_chunks": len(splitted_docs),
        "vectorstore_name": vs_name,
        "vectorstore_path": save_path,
        "created_at": datetime.utcnow(),
        "status": "active"
    }

    # Add custom metadata
    if custom_metadata:
        metadata["custom"] = custom_metadata

    result = collection.insert_one(metadata)
    metadata["_id"] = str(result.inserted_id)

    print(f"‚úÖ Metadata ƒë√£ l∆∞u v√†o MongoDB: {metadata['_id']}")

    report_progress("completed", 100)

    return save_path, metadata


# ======================
# Utility: List vectorstores
# ======================
def list_vectorstores(mongo_uri: str = "mongodb://localhost:27017/") -> List[Dict]:
    """L·∫•y danh s√°ch t·∫•t c·∫£ vectorstores"""
    client = MongoClient(mongo_uri)
    db = client["interviewer_ai"]
    collection = db["vectorstores"]

    vectorstores = list(collection.find({"status": "active"}).sort("created_at", -1))

    for vs in vectorstores:
        vs["_id"] = str(vs["_id"])

    return vectorstores


# ======================
# Utility: Delete vectorstore
# ======================
def delete_vectorstore(
        vectorstore_id: str,
        mongo_uri: str = "mongodb://localhost:27017/",
        remove_files: bool = True
) -> bool:
    """X√≥a vectorstore"""
    from bson import ObjectId

    client = MongoClient(mongo_uri)
    db = client["interviewer_ai"]
    collection = db["vectorstores"]

    vs = collection.find_one({"_id": ObjectId(vectorstore_id)})

    if not vs:
        return False

    # Remove files
    if remove_files and os.path.exists(vs["vectorstore_path"]):
        import shutil
        shutil.rmtree(vs["vectorstore_path"])
        print(f"üóëÔ∏è ƒê√£ x√≥a files t·∫°i: {vs['vectorstore_path']}")

    # Update status in DB
    collection.update_one(
        {"_id": ObjectId(vectorstore_id)},
        {"$set": {"status": "deleted", "deleted_at": datetime.utcnow()}}
    )

    print(f"‚úÖ ƒê√£ x√≥a vectorstore: {vectorstore_id}")
    return True


# ======================
# Example usage
# ======================
if __name__ == "__main__":
    pdf_file = "example.pdf"

    if os.path.exists(pdf_file):
        save_path, metadata = build_vectorstore(
            pdf_path=pdf_file,
            chunk_size=1600,
            chunk_overlap=400,
            model_name="intfloat/multilingual-e5-large-instruct",
            splitter_strategy="nltk",
            skip_duplicate=True,
            custom_metadata={"category": "java", "topic": "advanced"}
        )

        print("\n" + "=" * 50)
        print("Vectorstore ƒë√£ ƒë∆∞·ª£c t·∫°o th√†nh c√¥ng!")
        print(f"Path: {save_path}")
        print(f"Metadata ID: {metadata['_id']}")
    else:
        print(f"File {pdf_file} kh√¥ng t·ªìn t·∫°i")