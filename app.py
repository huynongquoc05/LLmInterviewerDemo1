from flask import Flask, request, jsonify, render_template, send_file
from pymongo import MongoClient
# Import refactored interviewer
from LLMInterviewer4 import (
    AdaptiveInterviewer,
    InterviewConfig, KnowledgeBuilder
)
from gtts import gTTS
import os

import uuid
import threading
import time
from datetime import datetime, timedelta
from werkzeug.utils import secure_filename
from flask import Response, stream_with_context

from extension import build_cv_vectorstore_from_candidates
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
# T·∫°o custom config n·∫øu c·∫ßn
custom_config = InterviewConfig(
    threshold_high=7.0,
    threshold_low=4.0,
    max_attempts_per_level=2,
    max_total_questions=8,
    max_upper_level=2,
    llm_temperature=0.5,
    max_memory_turns=6
)
print("üîß ƒêang kh·ªüi t·∫°o AdaptiveInterviewer to√†n c·ª•c...")

global_interviewer = AdaptiveInterviewer(

    config=custom_config,
    embeddings_model="intfloat/multilingual-e5-large-instruct",
    llm_model="gemini-2.5-flash",
    mongo_uri="mongodb://localhost:27017/"
)

print("‚úÖ AdaptiveInterviewer ƒë√£ ƒë∆∞·ª£c load (1 l·∫ßn duy nh·∫•t).")

cv_vectorstore_path = "NotUse/vector_db_csv"  # CV vectorstore c·ªë ƒë·ªãnh
# Import t·ª´ BuildVectorStores.py (c·∫£i ti·∫øn)
from BuildVectorStores import (
    build_vectorstore,
    list_vectorstores,
    delete_vectorstore,
    VALID_MODELS
)

# Configuration
UPLOAD_FOLDER = 'uploads'
ALLOWED_EXTENSIONS = {'pdf'}
MAX_FILE_SIZE = 50 * 1024 * 1024  # 50MB

if not os.path.exists(UPLOAD_FOLDER):
    os.makedirs(UPLOAD_FOLDER)



def allowed_file(filename):
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS


# K·∫øt n·ªëi MongoDB
client = MongoClient("mongodb://localhost:27017/")
db = client["interviewer_ai"]
collection = db["interview_results"]

# Kh·ªüi t·∫°o Flask + Interviewer
app = Flask(__name__)

app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER
app.config['MAX_CONTENT_LENGTH'] = MAX_FILE_SIZE

# Th∆∞ m·ª•c l∆∞u tr·ªØ audio t·∫°m th·ªùi
AUDIO_FOLDER = "temp_audio"
if not os.path.exists(AUDIO_FOLDER):
    os.makedirs(AUDIO_FOLDER)

# Dictionary ƒë·ªÉ l∆∞u tr·ªØ th√¥ng tin audio files
audio_cache = {}


def clean_old_audio_files():
    """X√≥a c√°c file audio c≈© h∆°n 1 gi·ªù"""
    try:
        now = datetime.now()
        for filename in os.listdir(AUDIO_FOLDER):
            file_path = os.path.join(AUDIO_FOLDER, filename)
            if os.path.isfile(file_path):
                file_time = datetime.fromtimestamp(os.path.getctime(file_path))
                if now - file_time > timedelta(hours=1):
                    os.remove(file_path)
                    print(f"ƒê√£ x√≥a file audio c≈©: {filename}")
    except Exception as e:
        print(f"L·ªói khi x√≥a file audio c≈©: {e}")

def cleanup_temp_files():
    """X√≥a t·∫•t c·∫£ file t·∫°m khi k·∫øt th√∫c ch∆∞∆°ng tr√¨nh"""
    try:
        for filename in os.listdir(AUDIO_FOLDER):
            file_path = os.path.join(AUDIO_FOLDER, filename)
            if os.path.isfile(file_path):
                os.remove(file_path)
                print(f"ƒê√£ x√≥a file t·∫°m: {filename}")
    except Exception as e:
        print(f"L·ªói khi x√≥a file t·∫°m: {e}")

import re
from bs4 import BeautifulSoup

def remove_code_blocks(text: str) -> str:
    """
    Lo·∫°i b·ªè c√°c kh·ªëi code HTML (<pre><code>...</code></pre>)
    v√† l√†m s·∫°ch c√°c th·∫ª HTML kh√°c ƒë·ªÉ t·∫°o n·ªôi dung g·ªçn g√†ng cho TTS.
    """
    if not text:
        return ""

    # B√≥c n·ªôi dung HTML
    soup = BeautifulSoup(text, "html.parser")

    # X√≥a to√†n b·ªô c√°c th·∫ª code block
    for code_block in soup.find_all(["pre", "code"]):
        code_block.decompose()

    # L·∫•y ph·∫ßn text c√≤n l·∫°i (ƒë√£ b·ªè HTML tags)
    clean_text = soup.get_text(separator=" ", strip=True)

    # X·ª≠ l√Ω g·ªçn c√°c kho·∫£ng tr·∫Øng, xu·ªëng d√≤ng th·ª´a
    clean_text = re.sub(r'\s+', ' ', clean_text)

    return clean_text.strip()

def create_audio_from_text(text, lang='vi'):
    """
    T·∫°o file audio t·ª´ text b·∫±ng gTTS

    Args:
        text (str): VƒÉn b·∫£n c·∫ßn chuy·ªÉn th√†nh gi·ªçng n√≥i
        lang (str): Ng√¥n ng·ªØ ('vi' cho ti·∫øng Vi·ªát, 'en' cho ti·∫øng Anh)

    Returns:
        str: T√™n file audio ƒë∆∞·ª£c t·∫°o
    """
    try:
        # T·∫°o t√™n file unique
        audio_id = str(uuid.uuid4())
        audio_filename = f"question_{audio_id}.mp3"
        audio_path = os.path.join(AUDIO_FOLDER, audio_filename)

        # X·ª≠ l√Ω text ƒë·ªÉ t·ªëi ∆∞u cho TTS
        # Lo·∫°i b·ªè c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát c√≥ th·ªÉ g√¢y l·ªói
        # L√†m s·∫°ch text
        clean_text = remove_code_blocks(text)
        clean_text = clean_text.replace("ü§ñ", "").strip()

        # T·∫°o audio b·∫±ng gTTS
        tts = gTTS(text=clean_text, lang=lang, slow=False)
        tts.save(audio_path)

        # L∆∞u th√¥ng tin v√†o cache
        audio_cache[audio_id] = {
            'filename': audio_filename,
            'path': audio_path,
            'created_at': datetime.now(),
            'text': clean_text
        }

        print(f"‚úÖ ƒê√£ t·∫°o audio: {audio_filename}")
        return audio_id

    except Exception as e:
        print(f"‚ùå L·ªói t·∫°o audio: {e}")
        return None


def detect_language(text):
    """
    Ph√°t hi·ªán ng√¥n ng·ªØ c·ªßa vƒÉn b·∫£n (ƒë∆°n gi·∫£n)
    """
    # Ki·ªÉm tra xem c√≥ k√Ω t·ª± ti·∫øng Vi·ªát kh√¥ng
    vietnamese_chars = "√†√°·∫£√£·∫°·∫±·∫Ø·∫≥·∫µ·∫∑·∫ß·∫•·∫©·∫´·∫≠√®√©·∫ª·∫Ω·∫π·ªÅ·∫ø·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç·ªì·ªë·ªï·ªó·ªô·ªù·ªõ·ªü·ª°·ª£√π√∫·ªß≈©·ª•·ª´·ª©·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµƒë"
    vietnamese_chars += vietnamese_chars.upper()

    if any(char in text for char in vietnamese_chars):
        return 'vi'
    else:
        return 'en'


@app.route("/")
def index():
    """Trang ch·ªß - c√≥ th·ªÉ t·∫°o landing page"""
    # Cleanup old audio files
    clean_old_audio_files()
    return render_template("home.html")  # T·∫°o trang ch·ªß ri√™ng n·∫øu c·∫ßn

@app.route("/interviewing")
def interviewing():
    """Trang ph·ªèng v·∫•n ch√≠nh"""
    clean_old_audio_files()
    return render_template("interviewing.html")


@app.route("/interviewing/vectorstores", methods=["GET"])
def get_available_vectorstores():
    """L·∫•y danh s√°ch vectorstores ƒë·ªÉ ch·ªçn"""
    try:
        vectorstores = list_vectorstores(mongo_uri="mongodb://localhost:27017/")

        # Format cho dropdown
        options = []
        for vs in vectorstores:
            options.append({
                "id": vs["_id"],
                "name": vs["vectorstore_name"],
                "path": vs["vectorstore_path"],
                "topic": vs.get("custom", {}).get("topic", "Unknown"),
                "pdf_file": vs["pdf_file"],
                "created_at": vs["created_at"],
                "num_chunks": vs["num_chunks"]
            })

        return jsonify({
            "success": True,
            "vectorstores": options
        })
    except Exception as e:
        return jsonify({
            "success": False,
            "error": str(e)
        }), 500



@app.route("/interviewing/answer", methods=["POST"])
def answer():
    data = request.json
    candidate, answer = data["candidate"], data["answer"]

    result = global_interviewer.submit_answer(candidate, answer)

    # N·∫øu ph·ªèng v·∫•n ch∆∞a k·∫øt th√∫c v√† c√≥ c√¢u h·ªèi ti·∫øp theo
    if not result.get("finished") and "next_question" in result:
        question_text = result["next_question"]
        lang = detect_language(question_text)

        # üîπ Ch·ªâ t·∫°o audio n·∫øu phase != closing
        if result.get("phase") != "closing":
            audio_id = create_audio_from_text(question_text, lang)
            if audio_id:
                result["audio_id"] = audio_id
                result["audio_url"] = f"/audio/{audio_id}"

    # L∆∞u k·∫øt qu·∫£ v√†o MongoDB n·∫øu ph·ªèng v·∫•n k·∫øt th√∫c
    if result.get("finished"):
        insert_result = collection.insert_one(result["summary"])
        result["summary"]["_id"] = str(insert_result.inserted_id)

    # üîπ ƒê·∫£m b·∫£o FE bi·∫øt phase hi·ªán t·∫°i
    result.setdefault("phase", "technical")

    return jsonify(result)


@app.route("/audio/<audio_id>")
def serve_audio(audio_id):
    """
    Endpoint ƒë·ªÉ ph·ª•c v·ª• file audio
    """
    try:
        if audio_id in audio_cache:
            audio_path = audio_cache[audio_id]['path']
            if os.path.exists(audio_path):
                return send_file(
                    audio_path,
                    mimetype="audio/mpeg",
                    as_attachment=False,
                    download_name=f"question_{audio_id}.mp3"
                )

        return jsonify({"error": "Audio file not found"}), 404

    except Exception as e:
        print(f"L·ªói khi ph·ª•c v·ª• audio: {e}")
        return jsonify({"error": "Error serving audio file"}), 500


@app.route("/audio/info/<audio_id>")
def audio_info(audio_id):
    """
    Endpoint ƒë·ªÉ l·∫•y th√¥ng tin v·ªÅ audio file
    """
    if audio_id in audio_cache:
        info = audio_cache[audio_id].copy()
        info['created_at'] = info['created_at'].isoformat()
        info.pop('path', None)  # Kh√¥ng tr·∫£ v·ªÅ ƒë∆∞·ªùng d·∫´n file
        return jsonify(info)

    return jsonify({"error": "Audio not found"}), 404


@app.route("/test-tts", methods=["POST"])
def test_tts():
    """
    Endpoint ƒë·ªÉ test t√≠nh nƒÉng TTS
    """
    data = request.json
    text = data.get("text", "Xin ch√†o, ƒë√¢y l√† b√†i test text-to-speech")
    lang = data.get("lang", "vi")

    audio_id = create_audio_from_text(text, lang)
    if audio_id:
        return jsonify({
            "success": True,
            "audio_id": audio_id,
            "audio_url": f"/audio/{audio_id}",
            "text": text,
            "language": lang
        })
    else:
        return jsonify({
            "success": False,
            "error": "Failed to create audio"
        }), 500


# Background task ƒë·ªÉ d·ªçn d·∫πp file audio ƒë·ªãnh k·ª≥
def cleanup_scheduler():
    """
    Ch·∫°y cleanup m·ªói 30 ph√∫t
    """
    while True:
        time.sleep(30 * 60)  # 30 ph√∫t
        clean_old_audio_files()

        # D·ªçn d·∫πp cache
        now = datetime.now()
        expired_keys = []
        for audio_id, info in audio_cache.items():
            if now - info['created_at'] > timedelta(hours=1):
                expired_keys.append(audio_id)

        for key in expired_keys:
            audio_cache.pop(key, None)

        if expired_keys:
            print(f"ƒê√£ x√≥a {len(expired_keys)} audio cache entries")


# ================================
# Embedding Routes
# ================================

@app.route("/embedding")
def embedding_page():
    """Trang qu·∫£n l√Ω vectorstore"""
    return render_template("embedding.html")


@app.route("/embedding/upload", methods=["POST"])
def upload_pdf():
    """
    Upload PDF v√† t·∫°o vectorstore v·ªõi Server-Sent Events ƒë·ªÉ report progress
    """
    if 'pdf_file' not in request.files:
        return jsonify({"error": "No file uploaded"}), 400

    file = request.files['pdf_file']

    if file.filename == '':
        return jsonify({"error": "No file selected"}), 400

    if not allowed_file(file.filename):
        return jsonify({"error": "Only PDF files allowed"}), 400

    # Get parameters
    chunk_size = int(request.form.get('chunk_size', 1600))
    chunk_overlap = int(request.form.get('chunk_overlap', 400))
    model_name = request.form.get('model_name', 'intfloat/multilingual-e5-large-instruct')
    splitter_strategy = request.form.get('splitter_strategy', 'nltk')

    # Validate model
    if model_name not in VALID_MODELS:
        return jsonify({"error": f"Invalid model: {model_name}"}), 400

    # Save uploaded file
    filename = secure_filename(file.filename)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    unique_filename = f"{timestamp}_{filename}"
    filepath = os.path.join(app.config['UPLOAD_FOLDER'], unique_filename)

    file.save(filepath)

    def generate():
        """Generator function cho SSE"""
        progress_data = {"stage": "init", "progress": 0}

        def progress_callback(stage, progress):
            progress_data['stage'] = stage
            progress_data['progress'] = progress

        try:
            # Send initial progress
            yield f"data: {json.dumps({'status': 'processing', 'stage': 'init', 'progress': 0})}\n\n"

            # Build vectorstore
            save_path, metadata = build_vectorstore(
                pdf_path=filepath,
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap,
                model_name=model_name,
                mongo_uri="mongodb://localhost:27017/",
                splitter_strategy=splitter_strategy,
                skip_duplicate=True,
                custom_metadata={
                    "uploaded_filename": filename,
                    "original_path": filepath
                },
                progress_callback=progress_callback
            )

            # Send progress updates
            yield f"data: {json.dumps({'status': 'processing', **progress_data})}\n\n"

            # Success
            yield f"data: {json.dumps({'status': 'completed', 'progress': 100, 'metadata': metadata})}\n\n"

        except Exception as e:
            # Error
            yield f"data: {json.dumps({'status': 'error', 'message': str(e)})}\n\n"

        finally:
            # Optionally delete uploaded file after processing
            # os.remove(filepath)
            pass

    return Response(
        stream_with_context(generate()),
        mimetype='text/event-stream',
        headers={
            'Cache-Control': 'no-cache',
            'X-Accel-Buffering': 'no'
        }
    )


@app.route("/embedding/list", methods=["GET"])
def list_vectorstores_route():
    """L·∫•y danh s√°ch t·∫•t c·∫£ vectorstores"""
    try:
        vectorstores = list_vectorstores(mongo_uri="mongodb://localhost:27017/")
        return jsonify({
            "success": True,
            "vectorstores": vectorstores,
            "count": len(vectorstores)
        })
    except Exception as e:
        return jsonify({
            "success": False,
            "error": str(e)
        }), 500


@app.route("/embedding/delete/<vectorstore_id>", methods=["DELETE"])
def delete_vectorstore_route(vectorstore_id):
    """X√≥a vectorstore"""
    try:
        success = delete_vectorstore(
            vectorstore_id=vectorstore_id,
            mongo_uri="mongodb://localhost:27017/",
            remove_files=True
        )

        if success:
            return jsonify({
                "success": True,
                "message": "Vectorstore deleted successfully"
            })
        else:
            return jsonify({
                "success": False,
                "message": "Vectorstore not found"
            }), 404

    except Exception as e:
        return jsonify({
            "success": False,
            "error": str(e)
        }), 500


@app.route("/embedding/models", methods=["GET"])
def get_models():
    """L·∫•y danh s√°ch models available"""
    return jsonify({
        "success": True,
        "models": VALID_MODELS
    })


@app.route("/embedding/info/<vectorstore_id>", methods=["GET"])
def get_vectorstore_info(vectorstore_id):
    """L·∫•y th√¥ng tin chi ti·∫øt c·ªßa m·ªôt vectorstore"""
    try:
        from bson import ObjectId
        from pymongo import MongoClient

        client = MongoClient("mongodb://localhost:27017/")
        db = client["interviewer_ai"]
        collection = db["vectorstores"]

        vs = collection.find_one({"_id": ObjectId(vectorstore_id)})

        if not vs:
            return jsonify({
                "success": False,
                "message": "Vectorstore not found"
            }), 404

        vs["_id"] = str(vs["_id"])

        return jsonify({
            "success": True,
            "vectorstore": vs
        })

    except Exception as e:
        return jsonify({
            "success": False,
            "error": str(e)
        }), 500


# ================================
# Additional Routes (Optional)
# ================================


# ================================
# MAIN
# ================================
# USAGE: Th√™m c√°c routes n√†y v√†o app.py hi·ªán t·∫°i
# ƒê·∫£m b·∫£o import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt ·ªü ƒë·∫ßu file
# ================================

#Code m·ªõi
# ================================
# Interview Session Management Routes
# Th√™m v√†o app.py sau c√°c routes hi·ªán t·∫°i
# ================================

from bson import ObjectId
from datetime import datetime
import json


# ================================
# SESSION MANAGEMENT
# ================================

@app.route("/interview_session")
def interview_session_page():
    """Trang qu·∫£n l√Ω bu·ªïi ph·ªèng v·∫•n"""
    return render_template("interview_session.html")


@app.route("/interview_session/create", methods=["POST"])
def create_interview_session():
    """
    T·∫°o bu·ªïi ph·ªèng v·∫•n m·ªõi v·ªõi:
    - Config parameters
    - Danh s√°ch th√≠ sinh
    - Knowledge base (vectorstore_id)
    - Topic v√† outline
    - Pre-load knowledge_text t·ª´ vectorstore
    """
    try:
        data = request.json

        # Validate required fields
        required_fields = ['session_name', 'config', 'candidates', 'vectorstore_id', 'topic']
        for field in required_fields:
            if field not in data:
                return jsonify({"success": False, "error": f"Missing field: {field}"}), 400

        # Get vectorstore info
        vs_collection = db["vectorstores"]
        vectorstore = vs_collection.find_one({"_id": ObjectId(data["vectorstore_id"])})

        if not vectorstore:
            return jsonify({"success": False, "error": "Vectorstore not found"}), 404

        knowledge_vectorstore_path = vectorstore["vectorstore_path"]
        # ‚úÖ 1. Sinh vectorstore cho danh s√°ch th√≠ sinh
        cv_vectorstore_path = build_cv_vectorstore_from_candidates(data["candidates"])

        # Pre-load knowledge text
        from LLMInterviewer4 import KnowledgeBuilder


        # # Load embeddings
        # embeddings = HuggingFaceEmbeddings(
        #     model_name="intfloat/multilingual-e5-large-instruct"
        # )

        # Load knowledge vectorstore
        knowledge_db = FAISS.load_local(
            knowledge_vectorstore_path,
            global_interviewer.embeddings,
            allow_dangerous_deserialization=True
        )

        # Build knowledge context
        kb = global_interviewer.knowledge_builder
        kb.knowledge_db = knowledge_db


        # Build knowledge text
        knowledge_text = kb.build_context(
            topic=data["topic"],
            outline=data.get("outline", None)
        )
        from extension import summarize_knowledge_with_llm

        report = summarize_knowledge_with_llm(
            knowledge_text,
            topic=data["topic"],
            outline=data.get("outline", []),
            llm=global_interviewer.llm
        )

        # Create session document
        session_doc = {
            "session_name": data["session_name"],
            "config": data["config"],
            "candidates": data["candidates"],  # List of {name, class, status: 'pending'}
            "cv_vectorstore_path": cv_vectorstore_path,  # ‚úÖ NEW
            "vectorstore_id": data["vectorstore_id"],
            "vectorstore_path": knowledge_vectorstore_path,
            "topic": data["topic"],
            "outline": data.get("outline", []),
            "knowledge_text": knowledge_text,  # Pre-loaded knowledge
            "knowledge_summary": report,

            "created_at": datetime.utcnow().isoformat(),
            "status": "active",  # active, completed
            "completed_count": 0,
            "total_count": len(data["candidates"])
        }

        # Save to MongoDB
        sessions_collection = db["interview_sessions_master"]
        result = sessions_collection.insert_one(session_doc)

        return jsonify({
            "success": True,
            "session_id": str(result.inserted_id),
            "message": "Bu·ªïi ph·ªèng v·∫•n ƒë√£ ƒë∆∞·ª£c t·∫°o th√†nh c√¥ng!"
        })

    except Exception as e:
        print(f"Error creating interview session: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({"success": False, "error": str(e)}), 500


@app.route("/interview_session/list", methods=["GET"])
def list_interview_sessions():
    """L·∫•y danh s√°ch t·∫•t c·∫£ bu·ªïi ph·ªèng v·∫•n"""
    try:
        sessions_collection = db["interview_sessions_master"]
        sessions = list(sessions_collection.find().sort("created_at", -1))

        # Convert ObjectId to string
        for session in sessions:
            session["_id"] = str(session["_id"])
            # Don't send knowledge_text in list (too large)
            session.pop("knowledge_text", None)

        return jsonify({
            "success": True,
            "sessions": sessions
        })

    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500


@app.route("/interview_session/detail/<session_id>")
def interview_session_detail(session_id):
    """Trang chi ti·∫øt bu·ªïi ph·ªèng v·∫•n"""

    return render_template("interview_session_detail.html", session_id=session_id)


@app.route("/interview_session/get/<session_id>", methods=["GET"])
def get_interview_session(session_id):
    """L·∫•y th√¥ng tin chi ti·∫øt bu·ªïi ph·ªèng v·∫•n"""
    try:
        sessions_collection = db["interview_sessions_master"]
        session = sessions_collection.find_one({"_id": ObjectId(session_id)})

        if not session:
            return jsonify({"success": False, "error": "Session not found"}), 404


        session["_id"] = str(session["_id"])
        # Don't send full knowledge_text unless requested
        # 2Ô∏è‚É£ Load CV vectorstore
        cv_path = session.get("cv_vectorstore_path")
        if cv_path:
            cv_db = FAISS.load_local(
                cv_path,
                global_interviewer.embeddings,
                allow_dangerous_deserialization=True
            )
            global_interviewer.cv_db = cv_db  # C·∫≠p nh·∫≠t vectorstore th√≠ sinh

        include_knowledge = request.args.get('include_knowledge', 'false') == 'true'
        if not include_knowledge:
            session.pop("knowledge_text", None)

        return jsonify({
            "success": True,
            "session": session
        })

    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500


@app.route("/interview_session/start_candidate", methods=["POST"])
def start_candidate_interview():
    """
    B·∫Øt ƒë·∫ßu ph·ªèng v·∫•n cho m·ªôt th√≠ sinh trong bu·ªïi ph·ªèng v·∫•n
    D√πng knowledge_text ƒë√£ preload trong session MongoDB.
    """
    global global_interviewer

    try:
        data = request.json
        session_id = data["session_id"]
        candidate_name = data["candidate_name"]
        candidate_class = data["candidate_class"]

        full_candidate_name = f"{candidate_name},{candidate_class}"

        sessions_collection = db["interview_sessions_master"]
        session = sessions_collection.find_one({"_id": ObjectId(session_id)})
        if not session:
            return jsonify({"error": "Session not found"}), 404

        config = InterviewConfig(**session["config"])
        global_interviewer.config = config

        # # N·∫øu vectorstore kh√°c, reload knowledge builder
        # if session["vectorstore_path"] != getattr(global_interviewer.knowledge_builder.knowledge_db, "index_to_docstore_id", None):
        #     from langchain_community.vectorstores import FAISS
        #     embeddings = global_interviewer.embeddings
        #     knowledge_db = FAISS.load_local(
        #         session["vectorstore_path"],
        #         embeddings,
        #         allow_dangerous_deserialization=True
        #     )
        #     global_interviewer.knowledge_builder = KnowledgeBuilder(knowledge_db)
        outline_summary = session.get("knowledge_summary", "")
        # G·ªçi start_interview()
        result = global_interviewer.start_interview(
            candidate_name=full_candidate_name,
            topic=session["topic"],
            outline=session.get("outline", []),
            knowledge_text=session.get("knowledge_text", ""),
            outline_summary=outline_summary
        )

        # TTS
        question = result["question"]
        lang = detect_language(question)
        audio_id = create_audio_from_text(question, lang)

        result.update({
            "success": True,
            "audio_id": audio_id,
            "audio_url": f"/audio/{audio_id}" if audio_id else None,
            "phase": result.get("phase", "warmup")  # üëà Th√™m d√≤ng n√†y
        })

        return jsonify(result)

    except Exception as e:
        print(f"Error starting candidate interview: {e}")
        import traceback; traceback.print_exc()
        return jsonify({"error": str(e)}), 500



@app.route("/interview_session/update_candidate_status", methods=["POST"])
def update_candidate_status():
    """C·∫≠p nh·∫≠t tr·∫°ng th√°i th√≠ sinh sau khi ho√†n th√†nh ph·ªèng v·∫•n"""
    try:
        data = request.json
        session_id = data["session_id"]
        candidate_name = data["candidate_name"]
        status = data["status"]  # 'completed', 'in_progress', etc.

        sessions_collection = db["interview_sessions_master"]

        # Update candidate status in array
        sessions_collection.update_one(
            {
                "_id": ObjectId(session_id),
                "candidates.name": candidate_name
            },
            {
                "$set": {
                    "candidates.$.status": status,
                    "candidates.$.completed_at": datetime.utcnow().isoformat()
                }
            }
        )

        # Update completed count if status is 'completed'
        if status == 'completed':
            session = sessions_collection.find_one({"_id": ObjectId(session_id)})
            completed_count = sum(1 for c in session["candidates"] if c.get("status") == "completed")

            sessions_collection.update_one(
                {"_id": ObjectId(session_id)},
                {
                    "$set": {
                        "completed_count": completed_count,
                        "status": "completed" if completed_count == session["total_count"] else "active"
                    }
                }
            )

        return jsonify({"success": True})

    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500


@app.route("/interview_session/delete/<session_id>", methods=["DELETE"])
def delete_interview_session(session_id):
    """X√≥a bu·ªïi ph·ªèng v·∫•n"""
    try:
        sessions_collection = db["interview_sessions_master"]
        result = sessions_collection.delete_one({"_id": ObjectId(session_id)})

        if result.deleted_count > 0:
            return jsonify({"success": True})
        else:
            return jsonify({"success": False, "error": "Session not found"}), 404

    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500


@app.route("/interview_session/export/<session_id>", methods=["GET"])
def export_session_results(session_id):
    """Export k·∫øt qu·∫£ bu·ªïi ph·ªèng v·∫•n ra CSV"""
    try:
        sessions_collection = db["interview_sessions_master"]
        session = sessions_collection.find_one({"_id": ObjectId(session_id)})

        if not session:
            return jsonify({"error": "Session not found"}), 404

        # Get all results for this session's candidates
        results_collection = db["interview_results"]
        results = []

        for candidate in session["candidates"]:
            candidate_full_name = f"{candidate['name']},{candidate['class']}"
            result = results_collection.find_one(
                {"candidate_info.name": candidate_full_name},
                sort=[("interview_stats.timestamp", -1)]
            )
            if result:
                results.append(result)

        # Generate CSV
        import csv
        from io import StringIO

        output = StringIO()
        writer = csv.writer(output)

        # Header
        writer.writerow([
            'T√™n', 'L·ªõp', 'ƒêi·ªÉm cu·ªëi c√πng', 'S·ªë c√¢u h·ªèi',
            'Tr√¨nh ƒë·ªô', 'Th·ªùi gian', 'Tr·∫°ng th√°i'
        ])

        # Data
        for result in results:
            name_parts = result["candidate_info"]["name"].split(",")
            writer.writerow([
                name_parts[0],
                name_parts[1] if len(name_parts) > 1 else "",
                result["interview_stats"].get("final_score", 0),
                result["interview_stats"].get("total_questions", 0),
                result["candidate_info"].get("classified_level", ""),
                result["interview_stats"].get("timestamp", ""),
                "Ho√†n th√†nh" if result else "Ch∆∞a ho√†n th√†nh"
            ])

        output.seek(0)

        return Response(
            output.getvalue(),
            mimetype="text/csv",
            headers={
                "Content-Disposition": f"attachment;filename=interview_results_{session_id}.csv"
            }
        )

    except Exception as e:
        return jsonify({"error": str(e)}), 500



if __name__ == "__main__":
    # Kh·ªüi ƒë·ªông background cleanup thread
    cleanup_thread = threading.Thread(target=cleanup_scheduler, daemon=True)
    cleanup_thread.start()

    print("üöÄ Server ƒëang kh·ªüi ƒë·ªông...")
    print("üîä Text-to-Speech ƒë√£ ƒë∆∞·ª£c k√≠ch ho·∫°t")
    print("üìÅ Audio files ƒë∆∞·ª£c l∆∞u t·∫°i:", AUDIO_FOLDER)

    app.run(debug=False, threaded=True)
    #X√≥a file t·∫°m khi k·∫øt th√∫c
    cleanup_temp_files()

